# Deep-Learning-Model-Compression-using-Pruning
Successfully implemented weight pruning on a Keras-tensorflow model trained for syntactic analysis using LSTM over the IMDB datasets.
Achieved a remarkable 70% model compression with only a 1.2% drop in accuracy, demonstrating a significant reduction in the model's size and computational requirements.
Conducted thorough experimentation and fine-tuning to determine the optimal pruning parameters, resulting in the highest possible compression while maintaining satisfactory accuracy.
Demonstrated proficiency in using state-of-the-art deep learning techniques to optimize model performance and efficiency.
Contributed to the development of cutting-edge natural language processing (NLP) technology by applying advanced machine learning techniques.
Collaborated effectively with team members to achieve project objectives and meet project timelines.
Documented experimental procedures, results, and conclusions clearly and concisely to facilitate knowledge transfer and future development.

#UPDATE:
Performed Knowledge distillation on BERT to reduce the size using quantization, pruning, and knowledge distillation to reduce the model size.
This is implemented for the downstream task of intent classification.


