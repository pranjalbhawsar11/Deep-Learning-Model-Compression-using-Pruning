# Deep-Learning-Model-Compression-using-Pruning
Successfully implemented weight pruning on a Keras-tensorflow model trained for syntactic analysis using LSTM over the IMDB datasets.
Achieved a remarkable 70% model compression with only a 1.2% drop in accuracy, demonstrating a significant reduction in the model's size and computational requirements.
Conducted thorough experimentation and fine-tuning to determine the optimal pruning parameters, resulting in the highest possible compression while maintaining satisfactory accuracy.
Demonstrated proficiency in using state-of-the-art deep learning techniques to optimize model performance and efficiency.
Contributed to the development of cutting-edge natural language processing (NLP) technology through the application of advanced machine learning techniques.
Collaborated effectively with team members to achieve project objectives and meet project timelines.
Documented experimental procedures, results, and conclusions in a clear and concise manner to facilitate knowledge transfer and future development.


